import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report
from sklearn.utils.class_weight import compute_class_weight
from category_encoders import TargetEncoder
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
import joblib

# ======================
# 1ï¸âƒ£ æ•°æ®è¯»å–ä¸åˆæ­¥æ¸…ç†
# ======================
df = pd.read_csv("train.csv")
original_df = df.copy()  # ä¿å­˜åŸå§‹æ•°æ®ç”¨äºåç»­å¤„ç†
df = df.drop(columns=["id"])
y = df["label"]

# ======================
# 2ï¸âƒ£ ç‰¹å¾åˆ†ç»„
# ======================
normal_cols = ["no_of_adults", "avg_price_per_room"]
longtail_cols = [
    "no_of_children", "no_of_weekend_nights", "no_of_week_nights",
    "lead_time", "no_of_previous_cancellations",
    "no_of_previous_bookings_not_canceled", "no_of_special_requests"
]
cat_cols = [
    "type_of_meal_plan", "required_car_parking_space",
    "room_type_reserved", "market_segment_type", "repeated_guest"
]
time_cols = ["arrival_year", "arrival_month", "arrival_date"]

# ======================
# 3ï¸âƒ£ æ—¶é—´ç‰¹å¾å‘¨æœŸåŒ–
# ======================
for col in ["arrival_month", "arrival_date"]:
    df[f"{col}_sin"] = np.sin(2 * np.pi * df[col] / df[col].max())
    df[f"{col}_cos"] = np.cos(2 * np.pi * df[col] / df[col].max())
df = df.drop(columns=time_cols)

# ======================
# 4ï¸âƒ£ é•¿å°¾åˆ†å¸ƒ log å¹³æ»‘
# ======================
df[longtail_cols] = df[longtail_cols].apply(lambda x: np.log1p(x))

# ======================
# 5ï¸âƒ£ æ•°å€¼ç‰¹å¾ç¼©æ”¾
# ======================
scaler_normal = StandardScaler()
df[normal_cols] = scaler_normal.fit_transform(df[normal_cols])

scaler_longtail = MinMaxScaler()
df[longtail_cols] = scaler_longtail.fit_transform(df[longtail_cols])

# ======================
# 6ï¸âƒ£ åˆ†ç±»ç‰¹å¾ç›®æ ‡ç¼–ç 
# ======================
encoder = TargetEncoder(cols=cat_cols)
df[cat_cols] = encoder.fit_transform(df[cat_cols], y)

# ======================
# 7ï¸âƒ£ ç”Ÿæˆæœ€ç»ˆç‰¹å¾é›†å¹¶ä¿å­˜
# ======================
X = df.drop(columns=["label"])
X.to_csv("preprocessed_features.csv", index=False)
y.to_csv("labels.csv", index=False)
print("âœ… é¢„å¤„ç†å®Œæˆå¹¶ä¿å­˜ä¸ºï¼špreprocessed_features.csv ä¸ labels.csv")
print("ç‰¹å¾ç»´åº¦:", X.shape)

# ======================
# 8ï¸âƒ£ è®­ç»ƒ/éªŒè¯é›†åˆ’åˆ†
# ======================
X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# ======================
# 9ï¸âƒ£ ç±»åˆ«æƒé‡è®¡ç®—
# ======================
class_weights = compute_class_weight(
    class_weight='balanced',
    classes=np.unique(y_train),
    y=y_train
)
class_weights = dict(enumerate(class_weights))
print("ğŸ“Š ç±»åˆ«æƒé‡:", class_weights)

# ======================
# ğŸ”Ÿ æ¨¡å‹æ„å»ºä¸è®­ç»ƒ
# ======================
model = Sequential([
    Dense(128, activation='relu', input_dim=X_train.shape[1]),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dropout(0.3),
    Dense(1, activation='sigmoid')
])

model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=50,
    batch_size=64,
    class_weight=class_weights,
    callbacks=[early_stop],
    verbose=1
)

# ======================
# 11ï¸âƒ£ æ¨¡å‹è¯„ä¼°ï¼ˆMacro F1ï¼‰
# ======================
y_pred_prob = model.predict(X_val)
y_pred = (y_pred_prob > 0.5).astype(int).flatten()

acc = accuracy_score(y_val, y_pred)
f1_macro = f1_score(y_val, y_pred, average='macro')

print("\nâœ… è¯„ä¼°ç»“æœ:")
print(f"Accuracy: {acc:.4f}")
print(f"Macro F1-score: {f1_macro:.4f}")
print("\nConfusion Matrix:\n", confusion_matrix(y_val, y_pred))
print("\nClassification Report:\n", classification_report(y_val, y_pred, digits=4))

# # ======================
# # 12ï¸âƒ£ ä¿å­˜æ¨¡å‹ä¸é¢„å¤„ç†å™¨
# # ======================
# model.save("booking_model.h5")
# joblib.dump(scaler_normal, "scaler_normal.pkl")
# joblib.dump(scaler_longtail, "scaler_longtail.pkl")
# joblib.dump(encoder, "target_encoder.pkl")
# print("\nğŸ’¾ æ¨¡å‹ä¸é¢„å¤„ç†å™¨å·²ä¿å­˜ï¼šbooking_model.h5 + é¢„å¤„ç†å™¨ .pkl æ–‡ä»¶")

# ======================
# 13ï¸âƒ£ åŠ è½½æµ‹è¯•é›†å¹¶è¿›è¡Œæ¨ç†
# ======================
print("\nğŸ” å¼€å§‹å¤„ç†æµ‹è¯•é›†...")
test_df = pd.read_csv("test.csv")
test_ids = test_df["id"].copy()  # ä¿å­˜IDç”¨äºè¾“å‡ºç»“æœ

# åº”ç”¨ç›¸åŒçš„æ•°æ®é¢„å¤„ç†æ­¥éª¤
# å¤åˆ¶æµ‹è¯•æ•°æ®ä»¥é¿å…ä¿®æ”¹åŸå§‹æ•°æ®
test_data = test_df.copy()

# åˆ é™¤ä¸éœ€è¦çš„åˆ—
test_data = test_data.drop(columns=["id"])

# æ—¶é—´ç‰¹å¾å‘¨æœŸåŒ–
for col in ["arrival_month", "arrival_date"]:
    test_data[f"{col}_sin"] = np.sin(2 * np.pi * test_data[col] / original_df[col].max())
    test_data[f"{col}_cos"] = np.cos(2 * np.pi * test_data[col] / original_df[col].max())
test_data = test_data.drop(columns=[col for col in time_cols if col in test_data.columns])

# é•¿å°¾åˆ†å¸ƒ log å¹³æ»‘
test_data[longtail_cols] = test_data[longtail_cols].apply(lambda x: np.log1p(x))

# æ•°å€¼ç‰¹å¾ç¼©æ”¾
test_data[normal_cols] = scaler_normal.transform(test_data[normal_cols])
test_data[longtail_cols] = scaler_longtail.transform(test_data[longtail_cols])

# åˆ†ç±»ç‰¹å¾ç›®æ ‡ç¼–ç 
test_data[cat_cols] = encoder.transform(test_data[cat_cols])

# è¿›è¡Œé¢„æµ‹
test_pred_prob = model.predict(test_data)
test_pred = (test_pred_prob > 0.5).astype(int).flatten()

# åˆ›å»ºç»“æœDataFrameå¹¶ä¿å­˜
results_df = pd.DataFrame({
    'id': test_ids,
    'label': test_pred
})

results_df.to_csv('test_predictions1.csv', index=False)
print("âœ… æµ‹è¯•é›†é¢„æµ‹å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ test_predictions1.csv")
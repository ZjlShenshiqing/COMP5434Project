ä½ å¥½ï¼è¿™æ˜¯ä¸€ä¸ªéå¸¸æ£’çš„ Python è„šæœ¬ï¼Œå®ƒä»é›¶å¼€å§‹ï¼ˆ"ç™½ç›’"ï¼‰å®ç°äº†ä¸€ä¸ªå¼ºå¤§çš„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œå«åš**æ¢¯åº¦æå‡å†³ç­–æ ‘ (GBDT)**ã€‚

### ğŸ“œ ä»£ç æ€»ä½“æ¦‚è¿°

ç®€å•æ¥è¯´ï¼Œè¿™æ®µä»£ç çš„ç›®çš„æ˜¯**è®­ç»ƒä¸€ä¸ªåˆ†ç±»æ¨¡å‹**ï¼Œç”¨æ¥æ ¹æ®ä¸€äº›ç‰¹å¾ï¼ˆæ¯”å¦‚å¹´é¾„ã€æ”¶å…¥ï¼‰æ¥é¢„æµ‹ä¸€ä¸ªäºŒåˆ†ç±»ç»“æœï¼ˆæ¯”å¦‚ï¼Œ"æ˜¯"æˆ–"å¦"ï¼Œ"1"æˆ–"0"ï¼‰ã€‚

å®ƒä¸ä¾èµ–ä»»ä½•é«˜çº§çš„æœºå™¨å­¦ä¹ åº“ï¼ˆåƒ `sklearn` æˆ– `xgboost`ï¼‰ï¼Œè€Œæ˜¯åªä½¿ç”¨åŸºç¡€çš„ `numpy` å’Œ `pandas` æ¥æ„å»ºæ•´ä¸ªæ¨¡å‹ã€‚è¿™å°±åƒ**æˆ‘ä»¬ä¸ä¹°ç°æˆçš„ä¹é«˜å¥—è£…ï¼Œè€Œæ˜¯è‡ªå·±è®¾è®¡å’Œåˆ¶é€ æ¯ä¸€å—ç§¯æœ¨ï¼Œæœ€åæ­å‡ºä¸€ä¸ªå¤æ‚çš„åŸå ¡**ã€‚

è¿™ä¸ªæ¨¡å‹çš„æ ¸å¿ƒæ€æƒ³æ˜¯â€œå›¢é˜Ÿåˆä½œâ€ï¼š

1.  å®ƒé¦–å…ˆåšä¸€ä¸ªéå¸¸ç²—ç³™çš„åˆå§‹çŒœæµ‹ã€‚
2.  ç„¶åï¼Œå®ƒè®­ç»ƒä¸€ä¸ªâ€œä¸“å®¶â€ï¼ˆä¸€æ£µå†³ç­–æ ‘ï¼‰ï¼Œè¿™ä¸ªä¸“å®¶çš„ä»»åŠ¡æ˜¯**ä¸“é—¨çº æ­£ä¸Šä¸€æ­¥çš„é”™è¯¯**ã€‚
3.  æ¥ç€ï¼Œå®ƒè®­ç»ƒç¬¬äºŒä¸ªä¸“å®¶ï¼Œè¿™ä¸ªä¸“å®¶ä¸“é—¨çº æ­£**ç¬¬ä¸€ä¸ªä¸“å®¶çº æ­£åè¿˜å‰©ä¸‹çš„é”™è¯¯**ã€‚
4.  å®ƒä¼šé‡å¤è¿™ä¸ªè¿‡ç¨‹å‡ ç™¾æ¬¡ï¼ˆæ¯”å¦‚ 300 æ£µæ ‘ï¼‰ï¼Œæ¯ä¸€æ£µæ–°æ ‘éƒ½å»ºç«‹åœ¨å‰é¢æ‰€æœ‰æ ‘çš„é›†ä½“æ™ºæ…§ä¹‹ä¸Šï¼Œä¸æ–­ä¼˜åŒ–ã€‚
5.  æœ€ç»ˆï¼Œæ¨¡å‹çš„é¢„æµ‹ç»“æœæ˜¯è¿™ä¸ªâ€œä¸“å®¶å›¢é˜Ÿâ€æ‰€æœ‰æˆå‘˜ï¼ˆæ‰€æœ‰æ ‘ï¼‰çš„æ„è§æ€»å’Œã€‚

è¿™æ®µä»£ç å®ç°äº†è¿™ä¸ªå›¢é˜Ÿï¼ˆ`GBDTWhiteBox` ç±»ï¼‰ã€æ¯ä¸ªä¸“å®¶ï¼ˆ`GBDTTree` ç±»ï¼‰ä»¥åŠæ‰€æœ‰éœ€è¦çš„è¾…åŠ©å·¥å…·ï¼ˆæ•°æ®å¤„ç†ã€æ€§èƒ½è¯„ä¼°ç­‰ï¼‰ã€‚

-----

### ğŸ—ºï¸ é€»è¾‘å…³ç³»ä¸ ASCII æµç¨‹å›¾

ä¸‹é¢æ˜¯è¿™ä¸ªè„šæœ¬åœ¨è¿è¡Œæ—¶ï¼Œä»å¼€å§‹åˆ°ç»“æŸçš„ä¸»è¦æ‰§è¡Œæµç¨‹ã€‚è¿™ä¸»è¦å‘ç”Ÿåœ¨ `main()` å‡½æ•°ä¸­ï¼š

```ascii
[ å¼€å§‹ ]
    â”‚
    â–¼
[ 1. è§£æå‘½ä»¤è¡Œå‚æ•° ]
(è·å– --train, --test, --out ç­‰æ–‡ä»¶çš„è·¯å¾„)
(è·å– --lr, --max_depth ç­‰æ¨¡å‹è¶…å‚æ•°)
    â”‚
    â–¼
[ 2. åŠ è½½è®­ç»ƒæ•°æ® ]
(è¯»å– train.csv æ–‡ä»¶)
    â”‚
    â–¼
[ 3. å‡†å¤‡æ•°æ® ]
( 3a. `detect_columns`: è¯†åˆ«ç‰¹å¾(X)å’Œç›®æ ‡(y) )
( 3b. `fill_missing`: å¡«å……ç¼ºå¤±å€¼ )
    â”‚
    â–¼
[ 4. ç‰¹å¾å·¥ç¨‹: OneHotSimple ]
( 4a. `enc.fit`: å­¦ä¹ æ‰€æœ‰åˆ†ç±»ç‰¹å¾çš„å€¼ )
( 4b. `enc.transform`: å°†æ–‡æœ¬ç±»ç‰¹å¾è½¬æ¢ä¸ºæ•°å­—(0/1) )
    â”‚
    â–¼
[ 5. åˆ’åˆ†æ•°æ®é›† ]
( `stratified_split`: å°†æ•°æ®åˆ†ä¸ºè®­ç»ƒé›†(tr)å’ŒéªŒè¯é›†(va) )
    â”‚
    â–¼
[ 6. è®­ç»ƒ GBDT æ¨¡å‹ ]
(è°ƒç”¨ `GBDTWhiteBox.fit(Xtr, ytr)` )
    â”‚
    â”‚  (å†…éƒ¨å¾ªç¯å¼€å§‹: è®­ç»ƒ M æ£µæ ‘)
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  â”‚ ( 6a. è®¡ç®—æ¢¯åº¦ g å’Œ h (å½“å‰é”™è¯¯) ) â”‚
    â”‚  â”‚ ( 6b. æ•°æ®é‡‡æ · (è¡Œå’Œåˆ—) )        â”‚
    â”‚  â”‚ ( 6c. è®­ç»ƒä¸€æ£µ `GBDTTree` )      â”‚
    â”‚  â”‚ ( 6d. å°†æ–°æ ‘åŠ å…¥å›¢é˜Ÿ )          â”‚
    â”‚  â”‚ ( 6e. æ›´æ–°æ€»é¢„æµ‹ F )            â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€( M æ¬¡å¾ªç¯ )â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
[ 7. éªŒè¯æ¨¡å‹ ]
( 7a. `gbdt.predict_proba(Xva)`: åœ¨éªŒè¯é›†ä¸Šé¢„æµ‹æ¦‚ç‡ )
( 7b. `tune_threshold`: æ‰¾åˆ°æœ€ä½³åˆ†ç±»é˜ˆå€¼ (å¦‚ 0.4) )
( 7c. è®¡ç®— Macro F1 åˆ†æ•°å¹¶æ‰“å°ç»“æœ )
    â”‚
    â–¼
[ 8. (å¯é€‰) å¯¼å‡ºæ¨¡å‹å’Œåˆ†æ? ]
(æ£€æŸ¥ `args.export_artifacts` æ˜¯å¦è¢«è®¾ç½®)
    â”‚
    â”œâ”€(æ˜¯)â”€â–º [ ä¿å­˜ç‰¹å¾é‡è¦æ€§ã€æ ‘ç»“æ„Jsonã€æ¨¡å‹Pklæ–‡ä»¶ ]
    â”‚
    â””â”€(å¦)â”€â–º [ è·³è¿‡ ]
    â”‚
    â–¼
[ 9. æ£€æŸ¥æµ‹è¯•æ–‡ä»¶æ˜¯å¦å­˜åœ¨? ]
    â”‚
    â”œâ”€(æ˜¯)â”€â–º [ 10. é¢„æµ‹æµ‹è¯•é›† ]
    â”‚          ( 10a. åŠ è½½ test.csv )
    â”‚          ( 10b. `fill_missing` + `enc.transform` )
    â”‚          ( 10c. `gbdt.predict_proba(Xtest)` )
    â”‚          ( 10d. ä½¿ç”¨æœ€ä½³é˜ˆå€¼è½¬æ¢ä¸º 0/1 )
    â”‚          ( 10e. ä¿å­˜åˆ° submission.csv )
    â”‚
    â””â”€(å¦)â”€â–º [ 11. æ‰“å°ä¿¡æ¯ï¼Œè·³è¿‡é¢„æµ‹ ]
    â”‚
    â–¼
[ ç»“æŸ ]
```

-----

### ğŸ§© åˆ†æ­¥æ‰§è¡Œçš„è§£é‡Š

ä¸‹é¢æˆ‘ä»¬æŠŠä»£ç æ‹†åˆ†æˆå°å—ï¼Œä¸€æ­¥ä¸€æ­¥æ¥çœ‹å®ƒä»¬æ˜¯åšä»€ä¹ˆçš„ã€‚

#### 1\. å¯¼å…¥å’Œè·¯å¾„è®¾ç½®

```python
import argparse, json, pickle
from pathlib import Path
from dataclasses import dataclass
import numpy as np
import pandas as pd

SCRIPT_DIR = Path(__file__).resolve().parent
DEFAULT_DATA_DIR = SCRIPT_DIR / "data"
DEFAULT_OUTPUT_DIR = SCRIPT_DIR / "output"
DEFAULT_DATA_DIR.mkdir(parents=True, exist_ok=True)
DEFAULT_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
```

**è¿™æ®µä»£ç çš„ä½œç”¨:**

  * **å¯¼å…¥å·¥å…·åŒ…**ï¼šå¯¼å…¥äº†æ‰€æœ‰éœ€è¦çš„ Python åº“ã€‚
      * `argparse`: ç”¨äºè§£æä»å‘½ä»¤è¡Œä¼ å…¥çš„å‚æ•°ï¼ˆæ¯”å¦‚æ–‡ä»¶åï¼‰ã€‚
      * `json`, `pickle`: ç”¨äºä¿å­˜æ¨¡å‹å’Œç»“æœã€‚
      * `pathlib`: ç”¨äºå¤„ç†æ–‡ä»¶å’Œæ–‡ä»¶å¤¹è·¯å¾„ã€‚
      * `dataclass`: ä¸€ä¸ªæ–¹ä¾¿åˆ›å»º"æ•°æ®å®¹å™¨"ç±»ï¼ˆå¦‚ `Node`ï¼‰çš„å·¥å…·ã€‚
      * `numpy`: æ ¸å¿ƒçš„æ•°å­¦è®¡ç®—åº“ã€‚
      * `pandas`: ç”¨äºè¯»å–å’Œå¤„ç† CSV æ•°æ®ã€‚
  * **è®¾ç½®é»˜è®¤è·¯å¾„**ï¼šå®ƒä¼šæ‰¾åˆ°å½“å‰è„šæœ¬æ‰€åœ¨çš„æ–‡ä»¶å¤¹ï¼ˆ`SCRIPT_DIR`ï¼‰ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šåˆ›å»ºé»˜è®¤çš„ `data` å’Œ `output` æ–‡ä»¶å¤¹ã€‚
  * **åˆ›å»ºæ–‡ä»¶å¤¹**ï¼š`mkdir(...)` ç¡®ä¿è¿™ä¸¤ä¸ªæ–‡ä»¶å¤¹ä¸€å®šå­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨å°±ä¼šè‡ªåŠ¨åˆ›å»ºå®ƒä»¬ã€‚

-----

#### 2\. è¾…åŠ©å‡½æ•°ï¼šåˆ†å±‚æŠ½æ ·

```python
def stratified_split(y, test_size=0.2, seed=42):
    rng = np.random.default_rng(seed)
    y = y.astype(int)
    pos = np.where(y==1)[0]; rng.shuffle(pos)
    neg = np.where(y==0)[0]; rng.shuffle(neg)
    npos_va = int(len(pos)*test_size); nneg_va = int(len(neg)*test_size)
    va = np.concatenate([pos[:npos_va], neg[:nneg_va]])
    tr = np.concatenate([pos[npos_va:], neg[nneg_va:]])
    rng.shuffle(va); rng.shuffle(tr)
    return tr, va
```

**è¿™æ®µä»£ç çš„ä½œç”¨:**

  * è¿™æ˜¯ä¸€ä¸ª**åˆ†å±‚æŠ½æ ·**å‡½æ•°ã€‚åœ¨åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œå¦‚æœ"ç±»åˆ«1"ï¼ˆæ­£æ ·æœ¬ï¼‰å¾ˆå°‘ï¼Œè€Œ"ç±»åˆ«0"ï¼ˆè´Ÿæ ·æœ¬ï¼‰å¾ˆå¤šï¼ˆè¿™å«"ç±»åˆ«ä¸å‡è¡¡"ï¼‰ï¼Œæˆ‘ä»¬å¸Œæœ›**è®­ç»ƒé›†**å’Œ**éªŒè¯é›†**éƒ½ä¿æŒè¿™ä¸ªæ¯”ä¾‹ã€‚
  * **æ‰§è¡Œæ­¥éª¤**ï¼š
    1.  `rng = ...`: åˆ›å»ºä¸€ä¸ªéšæœºæ•°ç”Ÿæˆå™¨ï¼Œ`seed=42` ä¿è¯æ¯æ¬¡è¿è¡Œçš„éšæœºç»“æœéƒ½ä¸€æ ·ã€‚
    2.  `pos = ...`, `neg = ...`: æ‰¾åˆ°æ‰€æœ‰æ­£æ ·æœ¬ (y=1) å’Œè´Ÿæ ·æœ¬ (y=0) çš„ç´¢å¼•ï¼ˆä½ç½®ï¼‰ã€‚
    3.  `rng.shuffle(pos)`: æ‰“ä¹±æ­£æ ·æœ¬çš„é¡ºåºã€‚
    4.  `npos_va = ...`: è®¡ç®—éªŒè¯é›†éœ€è¦å¤šå°‘ä¸ªæ­£æ ·æœ¬ï¼ˆä¾‹å¦‚ï¼Œæ€»å…±çš„20%ï¼‰ã€‚
    5.  `va = ...`: ä»æ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬ä¸­å„å–å‡º 20% åˆå¹¶ï¼Œä½œä¸ºéªŒè¯é›† (`va`) ç´¢å¼•ã€‚
    6.  `tr = ...`: æŠŠå‰©ä¸‹çš„ 80% åˆå¹¶ï¼Œä½œä¸ºè®­ç»ƒé›† (`tr`) ç´¢å¼•ã€‚
    7.  `return tr, va`: è¿”å›è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„ç´¢å¼•åˆ—è¡¨ã€‚

-----

#### 3\. è¾…åŠ©å‡½æ•°ï¼šSigmoid

```python
def sigmoid(x):
    # numerically stable
    out = np.empty_like(x, dtype=float)
    pos = x >= 0
    out[pos] = 1 / (1 + np.exp(-x[pos]))
    expx = np.exp(x[~pos])
    out[~pos] = expx / (1 + expx)
    return out
```

**è¿™æ®µä»£ç çš„ä½œç”¨:**

  * `sigmoid` æ˜¯ä¸€ä¸ªæ•°å­¦å‡½æ•°ï¼Œå®ƒå¯ä»¥æŠŠ**ä»»æ„æ•°å­—**ï¼ˆæ¨¡å‹çš„åŸå§‹è¾“å‡ºï¼Œå¯èƒ½æ˜¯ -5.0 æˆ– 3.2ï¼‰**å‹ç¼©åˆ° 0 å’Œ 1 ä¹‹é—´**ã€‚
  * åœ¨äºŒåˆ†ç±»ä¸­ï¼Œè¿™ä¸ª 0 åˆ° 1 ä¹‹é—´çš„å€¼å¯ä»¥è¢«çœ‹ä½œæ˜¯â€œ**é¢„æµ‹ä¸ºç±»åˆ«1çš„æ¦‚ç‡**â€ã€‚æ¯”å¦‚ 0.8 å°±ä»£è¡¨ 80% çš„æ¦‚ç‡æ˜¯ 1ã€‚
  * **å¤æ‚è¯­æ³•è§£é‡Š**ï¼š
      * `out = np.empty_like(x, ...)`: åˆ›å»ºä¸€ä¸ªå’Œè¾“å…¥ `x` å½¢çŠ¶ä¸€æ ·çš„æ–°æ•°ç»„ï¼Œç”¨æ¥å­˜æ”¾ç»“æœã€‚
      * `pos = x >= 0`: æ‰¾åˆ° `x` ä¸­æ‰€æœ‰å¤§äºç­‰äº 0 çš„ä½ç½®ã€‚
      * `out[pos] = ...`: å¯¹æ‰€æœ‰å¤§äºç­‰äº 0 çš„æ•°ï¼Œä½¿ç”¨æ ‡å‡† sigmoid å…¬å¼ $1 / (1 + e^{-x})$ã€‚
      * `out[~pos] = ...`: å¯¹æ‰€æœ‰å°äº 0 çš„æ•°ï¼ˆ`~pos` è¡¨ç¤º "ä¸åœ¨ pos é‡Œçš„"ï¼‰ï¼Œä½¿ç”¨ä¸€ä¸ªç­‰ä»·ä½†æ•°å€¼ä¸Šæ›´ç¨³å®šçš„å…¬å¼ $e^x / (1 + e^x)$ã€‚è¿™å¯ä»¥é˜²æ­¢ `np.exp` è®¡ç®—è¶…å¤§è´Ÿæ•°æ—¶å‡ºé”™ã€‚

-----

#### 4\. è¾…åŠ©å‡½æ•°ï¼šè®¡ç®—è¯„ä¼°æŒ‡æ ‡

```python
def confusion_matrix(y_true, y_pred):
    y_true = y_true.astype(int); y_pred = y_pred.astype(int)
    tp = int(((y_true==1)&(y_pred==1)).sum())
    tn = int(((y_true==0)&(y_pred==0)).sum())
    fp = int(((y_true==0)&(y_pred==1)).sum())
    fn = int(((y_true==1)&(y_pred==0)).sum())
    return np.array([[tn, fp],[fn, tp]])

def precision_recall_f1(y_true, y_pred):
    cm = confusion_matrix(y_true, y_pred)
    tn, fp, fn, tp = cm[0,0], cm[0,1], cm[1,0], cm[1,1]
    def prf(tp, fp, fn):
        p = tp/(tp+fp) if (tp+fp)>0 else 0.0
        r = tp/(tp+fn) if (tp+fn)>0 else 0.0
        f1 = 2*p*r/(p+r) if (p+r)>0 else 0.0
        return p,r,f1
    p1,r1,f1_1 = prf(tp, fp, fn)
    p0,r0,f1_0 = prf(tn, fn, fp)  # treat 0 as positive
    macro_f1 = (f1_0 + f1_1)/2.0
    acc = (tp+tn) / max(1,(tp+tn+fp+fn))
    return {"acc":acc,"macro_f1":macro_f1,"cm":cm.tolist(),
            "class1":{"precision":p1,"recall":r1,"f1":f1_1},
            "class0":{"precision":p0,"recall":r0,"f1":f1_0}}
```

**è¿™æ®µä»£ç çš„ä½œç”¨:**

  * `confusion_matrix` (æ··æ·†çŸ©é˜µ): è¿™æ˜¯è¯„ä¼°åˆ†ç±»æ¨¡å‹çš„åŸºç¡€ã€‚å®ƒç»Ÿè®¡äº†å››ç§æƒ…å†µï¼š
      * `tp` (True Positive): çœŸçš„ (True) æ˜¯ 1ï¼Œæ¨¡å‹ä¹Ÿé¢„æµ‹ (Positive) æ˜¯ 1ã€‚ **(çŒœå¯¹äº†)**
      * `tn` (True Negative): çœŸçš„ (True) æ˜¯ 0ï¼Œæ¨¡å‹ä¹Ÿé¢„æµ‹ (Negative) æ˜¯ 0ã€‚ **(çŒœå¯¹äº†)**
      * `fp` (False Positive): çœŸçš„ (False) æ˜¯ 0ï¼Œæ¨¡å‹å´é¢„æµ‹ (Positive) æ˜¯ 1ã€‚ **(çŒœé”™äº†ï¼Œ"è¯¯æŠ¥")**
      * `fn` (False Negative): çœŸçš„ (False) æ˜¯ 1ï¼Œæ¨¡å‹å´é¢„æµ‹ (Negative) æ˜¯ 0ã€‚ **(çŒœé”™äº†ï¼Œ"æ¼æŠ¥")**
  * `precision_recall_f1`: åŸºäºæ··æ·†çŸ©é˜µï¼Œè®¡ç®—æ›´é«˜çº§çš„æŒ‡æ ‡ã€‚
      * **Precision (ç²¾ç¡®ç‡)**: åœ¨æ‰€æœ‰æ¨¡å‹é¢„æµ‹ä¸º 1 çš„é‡Œé¢ï¼Œæœ‰å¤šå°‘æ˜¯çœŸçš„ 1ï¼Ÿï¼ˆå…³å¿ƒ"è¯¯æŠ¥"ï¼‰ã€‚
      * **Recall (å¬å›ç‡)**: åœ¨æ‰€æœ‰çœŸçš„æ˜¯ 1 çš„é‡Œé¢ï¼Œæ¨¡å‹æ‰¾å‡ºäº†å¤šå°‘ï¼Ÿï¼ˆå…³å¿ƒ"æ¼æŠ¥"ï¼‰ã€‚
      * **F1-Score**: ç²¾ç¡®ç‡å’Œå¬å›ç‡çš„å¹³è¡¡å¹³å‡æ•°ã€‚
      * **Macro F1**: åˆ†åˆ«è®¡ç®—ç±»åˆ« 1 çš„ F1 å’Œç±»åˆ« 0 çš„ F1ï¼Œç„¶åå–å¹³å‡ã€‚è¿™æ˜¯ä¸å‡è¡¡æ•°æ®é›†ä¸Šä¸€ä¸ªéå¸¸å¥½çš„è¯„ä¼°æŒ‡æ ‡ã€‚

-----

#### 5\. è¾…åŠ©å‡½æ•°ï¼šè°ƒæ•´é˜ˆå€¼

```python
def tune_threshold(y_true, proba, low=0.05, high=0.95, steps=19):
    best_t, best_f1 = 0.5, -1.0
    for t in np.linspace(low, high, steps):
        y_pred = (proba >= t).astype(int)
        f1 = precision_recall_f1(y_true, y_pred)["macro_f1"]
        if f1 > best_f1:
            best_f1, best_t = f1, float(t)
    return best_t, best_f1
```

**è¿™æ®µä»£ç çš„ä½œç”¨:**

  * æ¨¡å‹è¾“å‡ºçš„æ˜¯ 0 åˆ° 1 çš„æ¦‚ç‡ (proba)ã€‚æˆ‘ä»¬æ€ä¹ˆå†³å®š 0.4 ç®— 1 è¿˜æ˜¯ 0ï¼Ÿ 0.6 ç®— 1 è¿˜æ˜¯ 0ï¼Ÿ
  * é€šå¸¸é»˜è®¤é˜ˆå€¼ (threshold) æ˜¯ 0.5ã€‚ä½† 0.5 ä¸ä¸€å®šæ˜¯æœ€å¥½çš„ã€‚
  * è¿™ä¸ªå‡½æ•°ä¼š**å°è¯•**å¤šä¸ªä¸åŒçš„é˜ˆå€¼ï¼ˆæ¯”å¦‚ä» 0.05, 0.1, 0.15 ... åˆ° 0.95ï¼‰ã€‚
  * å¯¹æ¯ä¸ªé˜ˆå€¼ `t`ï¼Œå®ƒéƒ½è®¡ç®—ä¸€ä¸‹ Macro F1 åˆ†æ•°ã€‚
  * æœ€åï¼Œå®ƒè¿”å›é‚£ä¸ªèƒ½è®© Macro F1 åˆ†æ•°**æœ€é«˜**çš„é˜ˆå€¼ `best_t`ã€‚

-----

#### 6\. `OneHotSimple` ç±»ï¼šå­¦ä¹ åˆ†ç±»ç‰¹å¾

```python
class OneHotSimple:
    """Simple one-hot for categorical columns. Keeps all categories (or top-K), adds __OTHER__."""
    def __init__(self, max_categories=None, other="__OTHER__"):
        self.max_categories = max_categories
        self.other = other
        self.cat_cols = []
        self.num_cols = []
        self.col2cats = {}
        self.out_feature_names = []

    def fit(self, df, cat_cols, num_cols):
        self.cat_cols = list(cat_cols)
        self.num_cols = list(num_cols)
        feat_names = list(self.num_cols)
        for c in self.cat_cols:
            vc = df[c].astype(str).fillna(self.other).value_counts()
            if self.max_categories and len(vc) > self.max_categories:
                cats = sorted(vc.head(self.max_categories).index.tolist())
                if self.other not in cats: cats.append(self.other)
            else:
                cats = sorted(vc.index.tolist())
                if self.other not in cats: cats.append(self.other)
            self.col2cats[c] = cats
            feat_names += [f"{c}__{v}" for v in cats]
        self.out_feature_names = feat_names
        return self
```

**è¿™æ®µä»£ç çš„ä½œç”¨:**

  * è¿™æ˜¯æ•°æ®é¢„å¤„ç†çš„æ ¸å¿ƒã€‚æœºå™¨å­¦ä¹ æ¨¡å‹åªæ‡‚æ•°å­—ï¼Œä¸æ‡‚ "åŒ—äº¬"ã€"ä¸Šæµ·" è¿™æ ·çš„æ–‡æœ¬ã€‚
  * **One-Hot (ç‹¬çƒ­ç¼–ç )** å°±æ˜¯æŠŠæ–‡æœ¬å˜æˆæ•°å­—çš„æ–¹æ³•ã€‚
  * `__init__`: åˆå§‹åŒ–ã€‚
  * `fit` (å­¦ä¹ ):
      * å®ƒéå†æ‰€æœ‰è¢«å‘ŠçŸ¥æ˜¯â€œåˆ†ç±»â€çš„åˆ—ï¼ˆ`cat_cols`ï¼‰ã€‚
      * æ¯”å¦‚æœ‰ä¸€åˆ—å« "åŸå¸‚"ï¼Œå®ƒä¼šç»Ÿè®¡è¿™ä¸€åˆ—é‡Œå‡ºç°è¿‡çš„å€¼ï¼Œæ¯”å¦‚ `{"åŒ—äº¬": 50æ¬¡, "ä¸Šæµ·": 30æ¬¡, "å¹¿å·": 20æ¬¡}`ã€‚
      * å®ƒæŠŠè¿™äº›å€¼ï¼ˆ"åŒ—äº¬", "ä¸Šæµ·", "å¹¿å·"ï¼‰å­˜åˆ° `self.col2cats` å­—å…¸é‡Œã€‚
      * å®ƒè¿˜ä¼šç”Ÿæˆæ–°çš„ç‰¹å¾åå­—ï¼Œæ¯”å¦‚ `åŸå¸‚__åŒ—äº¬`, `åŸå¸‚__ä¸Šæµ·`, `åŸå¸‚__å¹¿å·`ï¼Œå¹¶å­˜å…¥ `self.out_feature_names`ã€‚
      * `__OTHER__` ç”¨äºå¤„ç†è®­ç»ƒæ—¶æ²¡è§è¿‡çš„æ–°å€¼æˆ–ç¼ºå¤±å€¼ã€‚

-----

#### 7\. `OneHotSimple` ç±»ï¼šè½¬æ¢æ•°æ®

```python
    def transform(self, df):
        X_num = df[self.num_cols].astype(float).to_numpy() if self.num_cols else np.zeros((len(df),0))
        mats = []
        for c in self.cat_cols:
            cats = self.col2cats[c]; cat2i = {v:i for i,v in enumerate(cats)}
            col = df[c].astype(str).fillna(self.other).to_numpy()
            idx = np.array([cat2i.get(v, cat2i[self.other]) for v in col], dtype=int)
            M = np.zeros((len(df), len(cats)), dtype=float)
            M[np.arange(len(df)), idx] = 1.0
            mats.append(M)
        X_cat = np.concatenate(mats, axis=1) if mats else np.zeros((len(df),0))
        X = np.concatenate([X_num, X_cat], axis=1)
        return X, list(self.out_feature_names)
```

**è¿™æ®µä»£ç çš„ä½œç”¨:**

  * `transform` (è½¬æ¢): è¿™æ˜¯åœ¨ `fit` å­¦ä¹ ä¹‹åï¼ŒçœŸæ­£åŠ¨æ‰‹è½¬æ¢æ•°æ®çš„æ­¥éª¤ã€‚
  * **æ‰§è¡Œæ­¥éª¤**:
    1.  `X_num = ...`: å…ˆæŠŠæ‰€æœ‰æ•°å€¼åˆ—ï¼ˆå¦‚ "å¹´é¾„"ï¼‰æ‹¿å‡ºæ¥ï¼Œè½¬æˆ `numpy` æ•°ç»„ã€‚
    2.  `for c in self.cat_cols:`: éå†æ‰€æœ‰åˆ†ç±»åˆ—ï¼ˆå¦‚ "åŸå¸‚"ï¼‰ã€‚
    3.  `cats = ...`: ä» `fit` çš„è®°å¿†ä¸­ï¼ˆ`self.col2cats`ï¼‰å–å‡º "åŸå¸‚" åˆ—çš„æ‰€æœ‰å¯èƒ½å€¼ï¼ˆ"åŒ—äº¬", "ä¸Šæµ·", "å¹¿å·"ï¼‰ã€‚
    4.  `M = np.zeros(...)`: åˆ›å»ºä¸€ä¸ªå…¨æ˜¯ 0 çš„çŸ©é˜µï¼Œè¡Œæ•°=æ•°æ®æ¡æ•°ï¼Œåˆ—æ•°=åŸå¸‚ä¸ªæ•° (3)ã€‚
    5.  `M[np.arange(len(df)), idx] = 1.0`: è¿™ä¸€æ­¥æ˜¯ One-Hot çš„æ ¸å¿ƒã€‚
          * å¦‚æœç¬¬ä¸€è¡Œæ•°æ®æ˜¯ "åŒ—äº¬"ï¼Œå®ƒå°±åœ¨ç¬¬ä¸€è¡Œçš„ "åŒ—äº¬" é‚£ä¸€åˆ—å¡« 1ã€‚
          * å¦‚æœç¬¬äºŒè¡Œæ•°æ®æ˜¯ "ä¸Šæµ·"ï¼Œå®ƒå°±åœ¨ç¬¬äºŒè¡Œçš„ "ä¸Šæµ·" é‚£ä¸€åˆ—å¡« 1ã€‚
          * ï¼ˆä¸€è¡Œæ•°æ®ä¸­ï¼Œåªæœ‰ä¸€ä¸ªæ˜¯ 1ï¼Œå…¶ä»–éƒ½æ˜¯ 0ï¼Œæ‰€ä»¥å« "ç‹¬çƒ­"ï¼‰ã€‚
    6.  `X = np.concatenate(...)`: æœ€åï¼ŒæŠŠæ•°å€¼åˆ—ï¼ˆ"å¹´é¾„"ï¼‰å’Œæ‰€æœ‰è½¬æ¢åçš„ç‹¬çƒ­çŸ©é˜µï¼ˆ"åŸå¸‚"ï¼‰æ‹¼åœ¨ä¸€èµ·ï¼Œå½¢æˆä¸€ä¸ªå·¨å¤§çš„ã€çº¯æ•°å­—çš„çŸ©é˜µ `X`ã€‚

-----

#### 8\. `Node` å’Œ `GBDTTree` ç±»ï¼šåˆå§‹åŒ–

```python
@dataclass
class Node:
    is_leaf: bool
    value: float = 0.0         # leaf value (logit increment)
    feature: int = None
    thr: float = None
    left: 'Node' = None
    right: 'Node' = None
    gain: float = 0.0
    n: int = 0

class GBDTTree:
    """A single regression tree trained with 2nd-order gain (Logloss)."""
    def __init__(self, max_depth=3, min_samples_leaf=20, lambda_l2=1.0,
                 gamma=0.0, n_bins=32, random_state=42, feature_subsample=None):
        self.max_depth = int(max_depth)
        self.min_samples_leaf = int(min_samples_leaf)
        self.lambda_l2 = float(lambda_l2)
        self.gamma = float(gamma)        # minimum gain to split
        self.n_bins = int(n_bins)
        self.rng = np.random.default_rng(random_state)
        self.feature_subsample = feature_subsample  # int or float or None
        self.root = None
        self.feature_gain_ = None
```

**è¿™æ®µä»£ç çš„ä½œç”¨:**

  * `Node` (èŠ‚ç‚¹) ç±»: è¿™æ˜¯ä¸€ä¸ª**æ•°æ®å®¹å™¨**ï¼Œç”¨æ¥è¡¨ç¤ºå†³ç­–æ ‘çš„ä¸€ä¸ªèŠ‚ç‚¹ã€‚
      * å¦‚æœ `is_leaf=True` (æ˜¯å¶å­)ï¼Œå®ƒå°±æœ‰ `value` (é¢„æµ‹å€¼)ã€‚
      * å¦‚æœ `is_leaf=False` (æ˜¯åˆ†æ”¯)ï¼Œå®ƒå°±æœ‰ `feature` (ç”¨å“ªä¸ªç‰¹å¾æé—®), `thr` (é˜ˆå€¼ï¼Œæ¯”å¦‚ "å¹´é¾„ \<= 30?"), `left` (å¦‚æœ"æ˜¯"è¯¥å»å“ª), `right` (å¦‚æœ"å¦"è¯¥å»å“ª)ã€‚
  * `GBDTTree` (GBDT æ ‘) ç±»: è¿™æ˜¯ GBDT æ¨¡å‹çš„**å•ä¸ªä¸“å®¶ï¼ˆä¸€æ£µæ ‘ï¼‰**ã€‚
  * `__init__` (åˆå§‹åŒ–): è®¾ç½®è¿™æ£µæ ‘çš„"è§„åˆ™"ï¼Œæ¯”å¦‚ï¼š
      * `max_depth=3`: æ ‘æœ€å¤šé•¿ 3 å±‚æ·±ï¼ˆé˜²æ­¢å¤ªå¤æ‚ï¼‰ã€‚
      * `min_samples_leaf=20`: ä¸€ä¸ªå¶å­èŠ‚ç‚¹å¿…é¡»è‡³å°‘åŒ…å« 20 ä¸ªæ ·æœ¬ã€‚
      * `lambda_l2`, `gamma`: æ­£åˆ™åŒ–å‚æ•°ï¼Œé˜²æ­¢æ¨¡å‹è¿‡æ‹Ÿåˆã€‚
      * `n_bins=32`: åœ¨å¯»æ‰¾æœ€ä½³åˆ†è£‚ç‚¹æ—¶ï¼Œä¸ç”¨å°è¯•æ‰€æœ‰å€¼ï¼Œè€Œæ˜¯æŠŠç‰¹å¾åˆ†æˆ 32 "ç®±"ï¼ŒåŠ å¿«é€Ÿåº¦ã€‚

-----

#### 9\. `GBDTTree` ç±»ï¼šå¯»æ‰¾åˆ†è£‚å€™é€‰ç‚¹

```python
    def _cand_thresholds(self, x):
        ux = np.unique(x)
        if len(ux) <= 2:
            return [0.5*(ux.min()+ux.max())]
        if len(ux) > self.n_bins:
            qs = np.linspace(0,1,self.n_bins+2)[1:-1]
            return np.unique(np.quantile(x, qs))
        return (ux[:-1] + ux[1:]) / 2.0
```

**è¿™æ®µä»£ç çš„ä½œç”¨:**

  * è¿™æ˜¯ä¸€ä¸ªå†…éƒ¨è¾…åŠ©å‡½æ•° `_cand_thresholds` (å€™é€‰é˜ˆå€¼)ã€‚
  * å½“æ ‘å†³å®šè¦ç”¨ "å¹´é¾„" ç‰¹å¾æ¥æé—®æ—¶ï¼Œå®ƒéœ€è¦å†³å®šé˜ˆå€¼ï¼šæ˜¯ "å¹´é¾„ \<= 30" è¿˜æ˜¯ "å¹´é¾„ \<= 40"ï¼Ÿ
  * å¦‚æœ "å¹´é¾„" æœ‰ 1000 ä¸ªä¸åŒçš„å€¼ï¼Œä¸€ä¸ªä¸€ä¸ªè¯•å¤ªæ…¢äº†ã€‚
  * è¿™ä¸ªå‡½æ•°å°±æ˜¯ç”¨æ¥**ç”Ÿæˆä¸€å°ç»„"å€™é€‰"é˜ˆå€¼**çš„ã€‚
  * **ç­–ç•¥**ï¼š
    1.  å¦‚æœç‰¹å¾å€¼ä¸å¤šï¼ˆæ¯”å¦‚ `len(ux) > self.n_bins` ä¸æˆç«‹ï¼‰ï¼Œå°±åœ¨æ¯ä¸¤ä¸ªç›¸é‚»å€¼ä¸­é—´å–ä¸€ä¸ªç‚¹ã€‚
    2.  å¦‚æœç‰¹å¾å€¼å¤ªå¤šï¼Œå®ƒå°±ä½¿ç”¨ `np.quantile` (åˆ†ä½æ•°) æ¥æ‰¾ `n_bins` (æ¯”å¦‚ 32) ä¸ªåˆ†å‰²ç‚¹ã€‚è¿™èƒ½ä¿è¯å€™é€‰ç‚¹åœ¨æ•°æ®åˆ†å¸ƒå¯†é›†çš„åœ°æ–¹æ›´å¯†ï¼Œåœ¨ç¨€ç–çš„åœ°æ–¹æ›´ç–ï¼Œéå¸¸é«˜æ•ˆã€‚

-----

#### 10\. `GBDTTree` ç±»ï¼šå¯»æ‰¾æœ€ä½³åˆ†è£‚ç‰¹å¾

```python
    def _best_split_feature(self, x, g, h):
        # sums on parent
        G = g.sum(); H = h.sum()
        base = (G*G)/(H + self.lambda_l2)

        best_gain, best_thr, best_left_mask = -1.0, None, None
        for t in self._cand_thresholds(x):
            L = x <= t
            nL = int(L.sum()); nR = len(x) - nL
            if nL < self.min_samples_leaf or nR < self.min_samples_leaf:
                continue
            GL, HL = g[L].sum(), h[L].sum()
            GR, HR = G-GL, H-HL
            gain = (GL*GL)/(HL + self.lambda_l2) + (GR*GR)/(HR + self.lambda_l2) - base - self.gamma
            if gain > best_gain:
                best_gain, best_thr, best_left_mask = float(gain), float(t), L
        return best_gain, best_thr, best_left_mask
```

**è¿™æ®µä»£ç çš„ä½œç”¨:**

  * è¿™æ˜¯**æ ‘ç”Ÿé•¿æœ€æ ¸å¿ƒ**çš„è®¡ç®—ï¼
  * å®ƒç”¨æ¥**è¯„ä¼°ä¸€ä¸ªç‰¹å¾ï¼ˆæ¯”å¦‚ "å¹´é¾„"ï¼‰ä¸Šçš„æ‰€æœ‰å€™é€‰åˆ†è£‚ç‚¹**ï¼Œå¹¶æ‰¾å‡ºæœ€å¥½çš„é‚£ä¸€ä¸ªã€‚
  * **è¾“å…¥**ï¼š`x` (å½“å‰ç‰¹å¾"å¹´é¾„"çš„æ‰€æœ‰å€¼), `g` (æ¢¯åº¦ï¼Œå¯ç†è§£ä¸º"æ¯ä¸ªæ ·æœ¬çš„é”™è¯¯"), `h` (æµ·æ£®å€¼ï¼Œå¯ç†è§£ä¸º"è®¡ç®—è¿™ä¸ªé”™è¯¯çš„éš¾åº¦æˆ–ç½®ä¿¡åº¦")ã€‚
  * **æ‰§è¡Œæ­¥éª¤**:
    1.  `G = g.sum()`: è®¡ç®—åˆ†è£‚å‰ï¼ˆåœ¨çˆ¶èŠ‚ç‚¹ï¼‰çš„æ€»é”™è¯¯ `G` å’Œæ€»éš¾åº¦ `H`ã€‚
    2.  `base = ...`: è®¡ç®—åˆ†è£‚å‰çš„â€œåˆ†æ•°â€ï¼ˆè¿™ä¸ªåˆ†æ•°è¶Šä½è¶Šå¥½ï¼Œä»£è¡¨è¶Šâ€œçº¯å‡€â€ï¼‰ã€‚
    3.  `for t in ...`: éå† `_cand_thresholds` ç”Ÿæˆçš„æ‰€æœ‰å€™é€‰é˜ˆå€¼ `t` (æ¯”å¦‚ "å¹´é¾„ \<= 30")ã€‚
    4.  `L = x <= t`: æ‰¾å‡ºå“ªäº›æ ·æœ¬æ»¡è¶³ "å¹´é¾„ \<= 30" (å·¦è¾¹, `L`)ã€‚
    5.  `if nL < ...`: æ£€æŸ¥åˆ†è£‚åçš„å·¦å³ä¸¤è¾¹æ˜¯å¦éƒ½æ»¡è¶³æœ€å°æ ·æœ¬æ•°ï¼ˆ`min_samples_leaf`ï¼‰è¦æ±‚ã€‚
    6.  `GL, HL = ...`: è®¡ç®—å·¦è¾¹é‚£å †æ ·æœ¬çš„ `G` å’Œ `H`ã€‚
    7.  `GR, HR = ...`: è®¡ç®—å³è¾¹é‚£å †æ ·æœ¬çš„ `G` å’Œ `H`ã€‚
    8.  `gain = ...`: **æ ¸å¿ƒå…¬å¼**ã€‚å®ƒåœ¨è®¡ç®—â€œåˆ†è£‚åçš„æ€»åˆ†æ•°â€å‡å»â€œåˆ†è£‚å‰çš„æ€»åˆ†æ•°â€ã€‚è¿™ä¸ª `gain` (å¢ç›Š) è¶Šå¤§ï¼Œè¯´æ˜è¿™æ¬¡åˆ†è£‚è¶Šå¥½ï¼
    9.  `if gain > best_gain`: å¦‚æœè¿™ä¸ªé˜ˆå€¼ `t` å¸¦æ¥çš„ `gain` æ˜¯è¿„ä»Šä¸ºæ­¢æœ€å¤§çš„ï¼Œå°±è®°ä¸‹å®ƒã€‚
    10. `return ...`: è¿”å›è¿™ä¸ªç‰¹å¾èƒ½å¸¦æ¥çš„**æœ€å¤§å¢ç›Š**ã€**æœ€ä½³é˜ˆå€¼**å’Œ**å·¦ä¾§æ ·æœ¬çš„æ©ç  (mask)**ã€‚

-----

#### 11\. `GBDTTree` ç±»ï¼šé€’å½’æ„å»ºæ ‘

```python
    def _build(self, X, g, h, depth):
        node = Node(is_leaf=True, value=(g.sum()/(h.sum()+self.lambda_l2)) if len(h)>0 else 0.0,
                    n=int(len(h)))
        if depth >= self.max_depth:
            return node
        if node.n < 2*self.min_samples_leaf:
            return node

        m = X.shape[1]
        # ... (çœç•¥äº† feature_subsample çš„ä»£ç ï¼Œä½œç”¨æ˜¯éšæœºé€‰ä¸€éƒ¨åˆ†ç‰¹å¾) ...
        feat_idx = np.arange(m) # (ç®€åŒ–å)

        best_feat, best_thr, best_gain, best_mask = None, None, -1.0, None
        for f in feat_idx:
            gain, thr, mask = self._best_split_feature(X[:,f], g, h)
            if gain > best_gain:
                best_gain, best_thr, best_feat, best_mask = gain, thr, int(f), mask

        if best_gain <= 1e-12 or best_mask is None:
            return node

        left = self._build(X[best_mask], g[best_mask], h[best_mask], depth+1)
        right = self._build(X[~best_mask], g[~best_mask], h[~best_mask], depth+1)

        parent = Node(is_leaf=False, feature=best_feat, thr=best_thr,
                        left=left, right=right, gain=best_gain, n=int(len(h)))
        return parent
```

**è¿™æ®µä»£ç çš„ä½œç”¨:**

  * `_build` æ˜¯ä¸€ä¸ª**é€’å½’**å‡½æ•°ï¼Œå®ƒæ˜¯çœŸæ­£**æ„å»º**æ ‘çš„å‡½æ•°ã€‚
  * **é€’å½’**ï¼šå°±åƒä¸€ä¸ªå‡½æ•°åœ¨è°ƒç”¨å®ƒè‡ªå·±æ¥è§£å†³ä¸€ä¸ªæ›´å°çš„é—®é¢˜ã€‚
  * **æ‰§è¡Œæ­¥éª¤**:
    1.  `node = Node(is_leaf=True, ...)`: **å…ˆå‡è®¾**è‡ªå·±æ˜¯ä¸€ä¸ªå¶å­èŠ‚ç‚¹ï¼Œå¹¶è®¡ç®—å‡ºè‡ªå·±çš„ `value`ï¼ˆè¿™ä¸ª value æ˜¯ GBDT ä¸­å¶å­èŠ‚ç‚¹çš„æ ‡å‡†è®¡ç®—å…¬å¼ï¼‰ã€‚
    2.  `if depth >= self.max_depth:`: **åœæ­¢æ¡ä»¶1**ã€‚å¦‚æœæ·±åº¦å¤ªæ·±äº†ï¼Œå°±åœæ­¢åˆ†è£‚ï¼Œè¿”å›è¿™ä¸ªå¶å­èŠ‚ç‚¹ã€‚
    3.  `if node.n < ...`: **åœæ­¢æ¡ä»¶2**ã€‚å¦‚æœæ ·æœ¬æ•°å¤ªå°‘ï¼ˆä¸å¤Ÿåˆ†è£‚æˆä¸¤ä¸ªæ»¡è¶³ `min_samples_leaf` çš„å­èŠ‚ç‚¹ï¼‰ï¼Œä¹Ÿåœæ­¢ï¼Œè¿”å›è¿™ä¸ªå¶å­èŠ‚ç‚¹ã€‚
    4.  `for f in feat_idx:`: éå†æ‰€æœ‰ï¼ˆæˆ–éšæœºé€‰æ‹©çš„ï¼‰ç‰¹å¾ã€‚
    5.  `gain, ... = self._best_split_feature(...)`: å¯¹æ¯ä¸ªç‰¹å¾ `f`ï¼Œè°ƒç”¨**æ­¥éª¤ 10** çš„å‡½æ•°ï¼Œæ‰¾åˆ°å®ƒæœ€å¥½çš„åˆ†è£‚æ–¹å¼ã€‚
    6.  `if gain > best_gain:`: æ‰¾å‡º**æ‰€æœ‰ç‰¹å¾ä¸­**ï¼Œé‚£ä¸ªèƒ½å¸¦æ¥**æœ€å¤§å¢ç›Š**çš„**æœ€ä½³ç‰¹å¾** (`best_feat`) å’Œ**æœ€ä½³é˜ˆå€¼** (`best_thr`)ã€‚
    7.  `if best_gain <= ...`: **åœæ­¢æ¡ä»¶3**ã€‚å¦‚æœæ‰¾åˆ°çš„æœ€å¥½åˆ†è£‚å¸¦æ¥çš„å¢ç›Š `gain` å‡ ä¹ä¸º 0ï¼Œè¯´æ˜å†åˆ†ä¸‹å»æ²¡æ„ä¹‰äº†ï¼Œè¿”å›å½“å‰çš„å¶å­èŠ‚ç‚¹ã€‚
    8.  `left = self._build(...)`: **é€’å½’è°ƒç”¨ï¼** æŠŠå±äºå·¦è¾¹ (`best_mask`) çš„æ•°æ®ï¼Œ**å†æ¬¡è°ƒç”¨ `_build` å‡½æ•°**ï¼Œè®©å®ƒå»æ„å»ºå·¦å­æ ‘ï¼ˆ`depth+1`ï¼‰ã€‚
    9.  `right = self._build(...)`: **é€’å½’è°ƒç”¨ï¼** æŠŠå±äºå³è¾¹ (`~best_mask`) çš„æ•°æ®ï¼Œ**å†æ¬¡è°ƒç”¨ `_build` å‡½æ•°**ï¼Œè®©å®ƒå»æ„å»ºå³å­æ ‘ã€‚
    10. `parent = Node(is_leaf=False, ...)`: **åˆå¹¶**ã€‚åˆ›å»ºä¸€ä¸ªæ–°çš„**éå¶å­**èŠ‚ç‚¹ (åˆ†æ”¯èŠ‚ç‚¹)ï¼ŒæŠŠ `left` å’Œ `right` ä½œä¸ºå®ƒçš„å·¦å³å­©å­ã€‚
    11. `return parent`: è¿”å›è¿™ä¸ªæ„å»ºå¥½çš„åˆ†æ”¯èŠ‚ç‚¹ã€‚

-----

#### 12\. `GBDTTree` ç±»ï¼šè®­ç»ƒå’Œä½¿ç”¨æ ‘

```python
    def fit(self, X, g, h):
        self.root = self._build(X, g, h, depth=0)
        # ... (çœç•¥äº† feature gain èšåˆçš„ä»£ç ) ...
        gains = {}
        def walk(n: Node):
            if n is None or n.is_leaf: return
            gains[n.feature] = gains.get(n.feature, 0.0) + n.gain * n.n
            walk(n.left); walk(n.right)
        walk(self.root)
        self.feature_gain_ = gains
        return self

    def _pred_one(self, x):
        n = self.root
        while not n.is_leaf:
            n = n.left if x[n.feature] <= n.thr else n.right
        return n.value

    def predict_value(self, X):
        return np.array([self._pred_one(x) for x in X], dtype=float)
```

**è¿™æ®µä»£ç çš„ä½œç”¨:**

  * `fit(self, X, g, h)` (è®­ç»ƒ):
      * è¿™æ˜¯è®­ç»ƒä¸€æ£µæ ‘çš„å…¥å£ã€‚å®ƒå¾ˆç®€å•ï¼Œå°±æ˜¯è°ƒç”¨**æ­¥éª¤ 11** çš„ `_build` å‡½æ•°ï¼Œä» `depth=0`ï¼ˆæ ¹èŠ‚ç‚¹ï¼‰å¼€å§‹ã€‚
      * `self.root` ä¼šä¿å­˜ `_build` è¿”å›çš„æ ‘çš„æ ¹èŠ‚ç‚¹ã€‚
      * `walk` å‡½æ•°ä¼šéå†æ•´æ£µæ ‘ï¼ŒæŠŠæ¯ä¸ªç‰¹å¾è´¡çŒ®çš„ `gain` ç´¯åŠ èµ·æ¥ï¼Œç”¨äºåç»­çš„â€œç‰¹å¾é‡è¦æ€§â€åˆ†æã€‚
  * `_pred_one(self, x)` (é¢„æµ‹ä¸€ä¸ªæ ·æœ¬):
      * å®ƒä» `self.root` (æ ‘æ ¹) å¼€å§‹ã€‚
      * `while not n.is_leaf`: åªè¦è¿˜æ²¡åˆ°å¶å­èŠ‚ç‚¹...
      * `n = n.left if x[n.feature] <= n.thr else n.right`: å°±è¿›è¡Œåˆ¤æ–­ã€‚æ¯”å¦‚ï¼Œå¦‚æœèŠ‚ç‚¹æ˜¯ "å¹´é¾„ \<= 30?"ï¼Œå®ƒå°±æ£€æŸ¥ `x` çš„"å¹´é¾„"ç‰¹å¾ (`x[n.feature]`) æ˜¯å¦å°äºç­‰äº 30 (`n.thr`)ï¼Œç„¶åèµ°å‘ `left` æˆ– `right`ã€‚
      * æœ€åï¼Œå®ƒä¼šåˆ°è¾¾ä¸€ä¸ªå¶å­èŠ‚ç‚¹ï¼Œå¹¶è¿”å›é‚£ä¸ªå¶å­çš„ `value`ã€‚
  * `predict_value(self, X)` (é¢„æµ‹æ‰€æœ‰æ ·æœ¬):
      * å¯¹è¾“å…¥çš„ `X` çŸ©é˜µä¸­çš„**æ¯ä¸€è¡Œ** `x`ï¼Œéƒ½è°ƒç”¨ä¸€æ¬¡ `_pred_one`ã€‚
      * è¿”å›ä¸€ä¸ªåŒ…å«æ‰€æœ‰é¢„æµ‹å€¼çš„æ•°ç»„ã€‚

-----

#### 13\. `GBDTWhiteBox` ç±»ï¼šåˆå§‹åŒ–

```python
class GBDTWhiteBox:
    def __init__(self, n_estimators=200, learning_rate=0.1,
                 max_depth=3, min_samples_leaf=20, lambda_l2=1.0, gamma=0.0,
                 subsample=1.0, colsample=1.0, n_bins=32, random_state=42):
        self.n_estimators = n_estimators
        self.learning_rate = learning_rate
        # ... (å­˜å‚¨æ‰€æœ‰è¶…å‚æ•°) ...
        self.rng = np.random.default_rng(random_state)

        self.trees_ = []
        self.init_score_ = 0.0
        self.feature_importances_ = None
```

**è¿™æ®µä»£ç çš„ä½œç”¨:**

  * è¿™æ˜¯**æ€»æ¨¡å‹**ï¼ˆ"ä¸“å®¶å›¢é˜Ÿ"ï¼‰çš„ç±»ã€‚
  * `__init__` (åˆå§‹åŒ–):
      * å®ƒä¿å­˜äº† GBDT æ¨¡å‹çš„**æ‰€æœ‰è¶…å‚æ•°**ã€‚
      * `n_estimators=200`: å›¢é˜Ÿé‡Œè¦æœ‰ 200 ä¸ªä¸“å®¶ï¼ˆæ ‘ï¼‰ã€‚
      * `learning_rate=0.1`: å­¦ä¹ ç‡ã€‚æ¯ä¸ªä¸“å®¶çš„æ„è§ï¼ˆ`value`ï¼‰éƒ½è¦ä¹˜ä»¥ 0.1 å†åŠ åˆ°æ€»åˆ†é‡Œï¼Œè¿™èƒ½è®©æ¨¡å‹æ›´ç¨³å®šï¼Œé˜²æ­¢å•ä¸ªä¸“å®¶â€œç”¨åŠ›è¿‡çŒ›â€ã€‚
      * `subsample=1.0`: è®­ç»ƒæ¯æ£µæ ‘æ—¶ç”¨å¤šå°‘æ¯”ä¾‹çš„æ ·æœ¬ï¼ˆ1.0 = å…¨éƒ¨ï¼‰ã€‚
      * `colsample=1.0`: è®­ç»ƒæ¯æ£µæ ‘æ—¶ç”¨å¤šå°‘æ¯”ä¾‹çš„ç‰¹å¾ï¼ˆ1.0 = å…¨éƒ¨ï¼‰ã€‚
      * `self.trees_ = []`: ä¸€ä¸ªåˆ—è¡¨ï¼Œç”¨æ¥å­˜æ”¾æ‰€æœ‰è®­ç»ƒå¥½çš„æ ‘ã€‚
      * `self.init_score_`: åˆå§‹é¢„æµ‹å€¼ï¼ˆ"ç²—ç³™çš„çŒœæµ‹"ï¼‰ã€‚

-----

#### 14\. `GBDTWhiteBox` ç±»ï¼šæ ¸å¿ƒè®­ç»ƒå¾ªç¯

```python
    def fit(self, X, y):
        n, m = X.shape
        # init with log-odds
        pos_rate = max(1e-6, min(1-1e-6, y.mean()))
        self.init_score_ = np.log(pos_rate/(1-pos_rate))

        F = np.full(n, self.init_score_, dtype=float)
        self.trees_ = []
        gain_sum = {}

        for t in range(self.n_estimators):
            p = sigmoid(F)
            g = y - p           # negative gradient for logistic
            h = p*(1-p)         # Hessian

            # ... (çœç•¥äº† row/col subsample çš„ä»£ç ) ...
            idx = np.arange(n)
            feat_sub = np.arange(m)
            X_sub = X[idx]
            
            tree = GBDTTree(max_depth=self.max_depth,
                           # ... (ä¼ å…¥æ‰€æœ‰æ ‘çš„è¶…å‚æ•°) ...
                           ).fit(X_sub, g[idx], h[idx])
            
            # ... (çœç•¥äº† pred_full çš„ä»£ç , ç”¨äºå¤„ç†åˆ—é‡‡æ ·) ...
            increment = self.learning_rate * tree.predict_value(X)
            F += increment

            # ... (çœç•¥äº† gain èšåˆçš„ä»£ç ) ...
            
            self.trees_.append({"tree": tree, "feat_sub": np.array(feat_sub, dtype=int)})
        
        # ... (çœç•¥äº† feature importances æ ‡å‡†åŒ–) ...
        return self
```

**è¿™æ®µä»£ç çš„ä½œç”¨:**

  * è¿™æ˜¯**æ•´ä¸ª GBDT æ¨¡å‹æœ€æ ¸å¿ƒçš„è®­ç»ƒé€»è¾‘**ï¼
  * **æ‰§è¡Œæ­¥éª¤**:
    1.  `self.init_score_ = ...`: è®¡ç®—ä¸€ä¸ª**åˆå§‹åˆ†æ•°** `F`ã€‚åœ¨é€»è¾‘å›å½’ (Logloss) ä¸­ï¼Œæœ€å¥½çš„åˆå§‹çŒœæµ‹æ˜¯æ‰€æœ‰æ ·æœ¬çš„"å¯¹æ•°å‡ ç‡ (log-odds)"ã€‚
    2.  `F = ...`: `F` æ˜¯æ¨¡å‹å½“å‰çš„**ç´¯ç§¯é¢„æµ‹åˆ†æ•°**ï¼ˆè¿˜æœªç»è¿‡ `sigmoid`ï¼‰ã€‚
    3.  `for t in range(self.n_estimators):`: **ä¸»å¾ªç¯**ï¼é‡å¤ 200 æ¬¡ï¼ˆ`n_estimators`ï¼‰ï¼Œæ¯æ¬¡è®­ç»ƒä¸€æ£µæ–°æ ‘ã€‚
    4.  `p = sigmoid(F)`: æŠŠå½“å‰çš„ç´¯ç§¯åˆ†æ•° `F` è½¬æ¢æˆ 0-1 ä¹‹é—´çš„**æ¦‚ç‡ `p`**ã€‚
    5.  `g = y - p`: **è®¡ç®—æ¢¯åº¦ (Gradient)**ã€‚è¿™å°±æ˜¯â€œé”™è¯¯â€ï¼
          * å¦‚æœçœŸå®å€¼ `y=1`ï¼Œæ¨¡å‹é¢„æµ‹ `p=0.8`ï¼Œé”™è¯¯ `g = 1 - 0.8 = 0.2` (æ­£é”™è¯¯)ã€‚
          * å¦‚æœçœŸå®å€¼ `y=0`ï¼Œæ¨¡å‹é¢„æµ‹ `p=0.3`ï¼Œé”™è¯¯ `g = 0 - 0.3 = -0.3` (è´Ÿé”™è¯¯)ã€‚
          * **ä¸‹ä¸€æ£µæ ‘çš„ä»»åŠ¡å°±æ˜¯å»æ‹Ÿåˆè¿™äº› `g` å€¼ï¼**
    6.  `h = p*(1-p)`: **è®¡ç®—æµ·æ£®å€¼ (Hessian)**ã€‚è¿™æ˜¯äºŒé˜¶ä¿¡æ¯ï¼ˆ"é”™è¯¯çš„éš¾åº¦"ï¼‰ï¼Œå®ƒè®© GBDT æ”¶æ•›æ›´å¿«ï¼ˆä¹Ÿå«ç‰›é¡¿æ³•ï¼‰ã€‚
    7.  `tree = GBDTTree(...)`: åˆ›å»ºä¸€ä¸ª**æ–°çš„ã€ç©ºç™½çš„** `GBDTTree` å®ä¾‹ï¼ˆ**æ­¥éª¤ 8**ï¼‰ã€‚
    8.  `.fit(X_sub, g[idx], h[idx])`: **è°ƒç”¨æ­¥éª¤ 12**ï¼Œè®©è¿™æ£µæ–°æ ‘å»**å­¦ä¹ å½“å‰çš„é”™è¯¯ `g` å’Œ `h`**ã€‚
    9.  `increment = ...`: ç”¨è¿™æ£µåˆšè®­ç»ƒå¥½çš„æ ‘ï¼Œé¢„æµ‹å®ƒå¯¹æ‰€æœ‰æ ·æœ¬çš„ä¿®æ­£å€¼ `value`ï¼Œå¹¶ä¹˜ä»¥**å­¦ä¹ ç‡ `learning_rate`**ã€‚
    10. `F += increment`: **æœ€å…³é”®çš„ä¸€æ­¥ï¼** æŠŠè¿™æ£µæ–°æ ‘çš„â€œä¿®æ­£æ„è§â€**æ·»åŠ **åˆ°æ€»åˆ†æ•° `F` ä¸Šã€‚
    11. `self.trees_.append(...)`: æŠŠè¿™æ£µè®­ç»ƒå¥½çš„æ ‘å­˜å…¥ "ä¸“å®¶å›¢é˜Ÿ" åˆ—è¡¨ã€‚
    12. å¾ªç¯ç»“æŸï¼Œæ¨¡å‹ï¼ˆ`self.trees_`ï¼‰å°±è®­ç»ƒå¥½äº†ã€‚

-----

#### 15\. `GBDTWhiteBox` ç±»ï¼šé¢„æµ‹

```python
    def predict_proba(self, X):
        F = np.full(X.shape[0], self.init_score_, dtype=float)
        for item in self.trees_:
            tree = item["tree"]; feat_sub = item["feat_sub"]
            Fx = tree.predict_value(X[:, feat_sub]) if len(feat_sub)!=X.shape[1] else tree.predict_value(X)
            F += self.learning_rate * Fx
        return sigmoid(F)
```

**è¿™æ®µä»£ç çš„ä½œç”¨:**

  * `predict_proba` (é¢„æµ‹æ¦‚ç‡): å½“æ¨¡å‹è®­ç»ƒå¥½åï¼Œç”¨å®ƒæ¥é¢„æµ‹æ–°æ•°æ®ã€‚
  * **æ‰§è¡Œæ­¥éª¤**:
    1.  `F = ...`: å…ˆä»**åˆå§‹åˆ†æ•°** `init_score_` å¼€å§‹ã€‚
    2.  `for item in self.trees_:`: éå†"ä¸“å®¶å›¢é˜Ÿ"ï¼ˆæ‰€æœ‰è®­ç»ƒå¥½çš„æ ‘ï¼‰ã€‚
    3.  `Fx = tree.predict_value(...)`: è®©**æ¯ä¸€æ£µæ ‘** `tree` éƒ½å¯¹æ–°æ•°æ® `X` è¿›è¡Œé¢„æµ‹ï¼ˆ**è°ƒç”¨æ­¥éª¤ 12**ï¼‰ã€‚
    4.  `F += self.learning_rate * Fx`: æŠŠ**æ¯ä¸€æ£µæ ‘**çš„é¢„æµ‹å€¼ï¼ˆä¹˜ä»¥å­¦ä¹ ç‡ï¼‰**ç´¯åŠ **åˆ°æ€»åˆ†æ•° `F` ä¸Šã€‚
    5.  `return sigmoid(F)`: **æœ€å**ï¼ŒæŠŠè¿™ä¸ªç´¯åŠ çš„æ€»åˆ†æ•° `F` é€šè¿‡ `sigmoid` å‡½æ•°ï¼ˆ**æ­¥éª¤ 3**ï¼‰ï¼Œè½¬æ¢æˆ 0 åˆ° 1 ä¹‹é—´çš„æ¦‚ç‡ï¼Œå¹¶è¿”å›ã€‚

-----

#### 16\. `main` å‡½æ•°ï¼šæ•°æ®å‡†å¤‡

```python
def main():
    # ... (ap = argparse.ArgumentParser()... )
    # ... (è§£ææ‰€æœ‰å‘½ä»¤è¡Œå‚æ•°, å¦‚ args.train, args.lr ç­‰) ...
    # ... (è®¾ç½® train_path, test_path, out_path) ...

    # Load data
    df = pd.read_csv(train_path)
    Xdf, y, num_cols, cat_cols = detect_columns(df, target=args.target)
    Xdf = fill_missing(Xdf, num_cols, cat_cols)

    # Fit encoder
    enc = OneHotSimple(max_categories=None, other="__OTHER__")
    enc.fit(Xdf, cat_cols=cat_cols, num_cols=num_cols)
    X, feat_names = enc.transform(Xdf)
```

**è¿™æ®µä»£ç çš„ä½œç”¨:**

  * `main` å‡½æ•°æ˜¯è¿™ä¸ªè„šæœ¬çš„**ä¸»å…¥å£**ã€‚
  * **è§£æå‚æ•°**: `argparse` éƒ¨åˆ†ï¼ˆå·²çœç•¥ï¼‰ä¼šè¯»å–ç”¨æˆ·åœ¨å‘½ä»¤è¡Œè¾“å…¥çš„æŒ‡ä»¤ï¼Œæ¯”å¦‚ç”¨å“ªä¸ªè®­ç»ƒæ–‡ä»¶ã€å­¦ä¹ ç‡è®¾ä¸ºå¤šå°‘ã€‚
  * **åŠ è½½æ•°æ®**: `pd.read_csv(train_path)` è¯»å–è®­ç»ƒ CSV æ–‡ä»¶ã€‚
  * **æ•°æ®å‡†å¤‡**:
      * `detect_columns`:ï¼ˆä¸€ä¸ªè¾…åŠ©å‡½æ•°ï¼‰è‡ªåŠ¨åŒºåˆ†å“ªäº›æ˜¯ç‰¹å¾ (X)ï¼Œå“ªäº›æ˜¯ç›®æ ‡ (y)ï¼Œå“ªäº›æ˜¯æ•°å€¼ (num\_cols)ï¼Œå“ªäº›æ˜¯åˆ†ç±» (cat\_cols)ã€‚
      * `fill_missing`:ï¼ˆä¸€ä¸ªè¾…åŠ©å‡½æ•°ï¼‰å¡«å……æ•°æ®ä¸­çš„ç¼ºå¤±å€¼ã€‚
  * **ç‰¹å¾å·¥ç¨‹**:
      * `enc = OneHotSimple(...)`: åˆ›å»ºä¸€ä¸ªç‹¬çƒ­ç¼–ç å™¨å®ä¾‹ï¼ˆ**æ­¥éª¤ 6**ï¼‰ã€‚
      * `enc.fit(...)`: **å­¦ä¹ **è®­ç»ƒæ•°æ®ä¸­çš„åˆ†ç±»ç‰¹å¾ã€‚
      * `enc.transform(Xdf)`: **è½¬æ¢**ï¼ˆ**æ­¥éª¤ 7**ï¼‰æ•°æ®ï¼ŒæŠŠ `Xdf` (Pandas DataFrame) å˜æˆ `X` (çº¯æ•°å­—çš„ Numpy æ•°ç»„)ã€‚

-----

#### 17\. `main` å‡½æ•°ï¼šè®­ç»ƒå’ŒéªŒè¯

```python
    # Split
    tr_idx, va_idx = stratified_split(y, test_size=0.2, seed=args.seed)
    Xtr, ytr = X[tr_idx], y[tr_idx]
    Xva, yva = X[va_idx], y[va_idx]

    # Train GBDT
    gbdt = GBDTWhiteBox(n_estimators=args.n_estimators, learning_rate=args.lr,
                        # ... (ä¼ å…¥æ‰€æœ‰æ¥è‡ª args çš„è¶…å‚æ•°) ...
                        ).fit(Xtr, ytr)

    # Validation + threshold tuning
    proba_va = gbdt.predict_proba(Xva)
    best_t, best_f1 = tune_threshold(yva, proba_va)
    ypred_va = (proba_va >= best_t).astype(int)
    rpt = precision_recall_f1(yva, ypred_va)
    print("="*60)
    print(f"[GBDT-WhiteBox] Validation Macro F1: {rpt['macro_f1']:.6f} @ threshold={best_t:.2f}")
    # ... (æ‰“å°å…¶ä»–æŒ‡æ ‡) ...
```

**è¿™æ®µä»£ç çš„ä½œç”¨:**

  * **æ‹†åˆ†æ•°æ®**:
      * `stratified_split` (**æ­¥éª¤ 2**) æŠŠæ•°æ® `X` å’Œ `y` æ‹†åˆ†æˆè®­ç»ƒé›† (`Xtr`, `ytr`) å’ŒéªŒè¯é›† (`Xva`, `yva`)ã€‚
  * **è®­ç»ƒæ¨¡å‹**:
      * `gbdt = GBDTWhiteBox(...)`: åˆ›å»º GBDT æ¨¡å‹çš„å®ä¾‹ï¼ˆ**æ­¥éª¤ 13**ï¼‰ï¼ŒæŠŠæ‰€æœ‰å‘½ä»¤è¡Œä¼ å…¥çš„è¶…å‚æ•°ï¼ˆå¦‚ `args.lr`ï¼‰éƒ½ä¼ è¿›å»ã€‚
      * `.fit(Xtr, ytr)`: **è°ƒç”¨æ­¥éª¤ 14**ï¼Œä½¿ç”¨**è®­ç»ƒé›†** (`Xtr`, `ytr`) æ¥è®­ç»ƒæ¨¡å‹ã€‚
  * **éªŒè¯æ¨¡å‹**:
      * `proba_va = gbdt.predict_proba(Xva)`: **è°ƒç”¨æ­¥éª¤ 15**ï¼Œåœ¨**éªŒè¯é›†** (`Xva`) ä¸Šè¿›è¡Œé¢„æµ‹ï¼Œå¾—åˆ°æ¦‚ç‡ã€‚
      * `best_t, ... = tune_threshold(...)`: **è°ƒç”¨æ­¥éª¤ 5**ï¼Œæ‰¾åˆ°åœ¨éªŒè¯é›†ä¸Š F1 åˆ†æ•°æœ€é«˜çš„é‚£ä¸ªé˜ˆå€¼ã€‚
      * `rpt = precision_recall_f1(...)`: **è°ƒç”¨æ­¥éª¤ 4**ï¼Œè®¡ç®—æœ€ç»ˆçš„ F1 åˆ†æ•°ã€å‡†ç¡®ç‡ã€æ··æ·†çŸ©é˜µç­‰ã€‚
      * `print(...)`: æŠŠéªŒè¯ç»“æœæ‰“å°å‡ºæ¥ã€‚

-----

#### 18\. `main` å‡½æ•°ï¼šå¯¼å‡ºå’Œé¢„æµ‹

```python
    # Optional exports: feature importance, trees, model
    if args.export_artifacts:
        # ... (ä¿å­˜ç‰¹å¾é‡è¦æ€§ã€æ ‘çš„ JSON ç»“æ„ã€æ¨¡å‹ pkl æ–‡ä»¶) ...
        print(f"[INFO] Saved model artifacts to: {artifact_dir}")

    # Predict test
    if test_path and test_path.exists():
        tdf = pd.read_csv(test_path).copy()
        # ... (æ£€æŸ¥ "id" åˆ—) ...
        Xtest_df = tdf.drop(...)
        Xtest_df = fill_missing(Xtest_df, num_cols, cat_cols)
        Xtest, _ = enc.transform(Xtest_df)
        proba_te = gbdt.predict_proba(Xtest)
        ypred_te = (proba_te >= best_t).astype(int)
        out_path.parent.mkdir(parents=True, exist_ok=True)
        pd.DataFrame({"id": tdf["id"], args.target: ypred_te}).to_csv(out_path, index=False)
        print(f"[INFO] Submission file saved to: {out_path}")
    
    # ... (å¤„ç† test æ–‡ä»¶ä¸å­˜åœ¨çš„æƒ…å†µ) ...

if __name__ == "__main__":
    main()
```

**è¿™æ®µä»£ç çš„ä½œç”¨:**

  * **å¯¼å‡ºå·¥ä»¶ (Artifacts)**ï¼šå¦‚æœç”¨æˆ·åœ¨å‘½ä»¤è¡Œè®¾ç½®äº† `--export_artifacts`ï¼Œå°±æŠŠè®­ç»ƒå¥½çš„æ¨¡å‹ã€ç‰¹å¾é‡è¦æ€§æ’åã€æ‰€æœ‰æ ‘çš„ç»“æ„ç­‰ï¼Œä¿å­˜ä¸ºæ–‡ä»¶ï¼Œæ–¹ä¾¿åç»­åˆ†æã€‚
  * **é¢„æµ‹æµ‹è¯•é›†**:
    1.  æ£€æŸ¥æµ‹è¯•æ–‡ä»¶ (`test.csv`) æ˜¯å¦å­˜åœ¨ã€‚
    2.  `tdf = pd.read_csv(...)`: åŠ è½½æµ‹è¯•æ•°æ®ã€‚
    3.  `fill_missing(...)`: ç”¨**è®­ç»ƒé›†**çš„ç»Ÿè®¡å€¼ï¼ˆæ¯”å¦‚ä¸­ä½æ•°ï¼‰æ¥å¡«å……æµ‹è¯•é›†çš„ç¼ºå¤±å€¼ã€‚
    4.  `Xtest, _ = enc.transform(Xtest_df)`: **é‡è¦ï¼** ä½¿ç”¨**ä¹‹å‰ `fit` è¿‡çš„**ç¼–ç å™¨ `enc` æ¥è½¬æ¢æµ‹è¯•é›†ï¼ˆ**æ­¥éª¤ 7**ï¼‰ã€‚è¿™ä¿è¯äº† "åŒ—äº¬" åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­è¢«æ˜ å°„åˆ°åŒä¸€ä¸ªæ•°å­—ã€‚
    5.  `proba_te = gbdt.predict_proba(Xtest)`: **è°ƒç”¨æ­¥éª¤ 15**ï¼Œç”¨è®­ç»ƒå¥½çš„ `gbdt` æ¨¡å‹é¢„æµ‹æµ‹è¯•é›†æ¦‚ç‡ã€‚
    6.  `ypred_te = (proba_te >= best_t).astype(int)`: ä½¿ç”¨**åœ¨éªŒè¯é›†ä¸Šæ‰¾åˆ°çš„æœ€ä½³é˜ˆå€¼ `best_t`**ï¼Œå°†æ¦‚ç‡è½¬æ¢ä¸º 0 æˆ– 1ã€‚
    7.  `pd.DataFrame(...).to_csv(...)`: åˆ›å»ºä¸€ä¸ªåŒ…å« `id` å’Œé¢„æµ‹ç»“æœçš„ `DataFrame`ï¼Œå¹¶ä¿å­˜ä¸º `submission.csv`ã€‚
  * `if __name__ == "__main__":`: è¿™æ˜¯ Python çš„æ ‡å‡†å†™æ³•ï¼Œæ„æ€æ˜¯â€œå½“è¿™ä¸ªæ–‡ä»¶è¢«ç›´æ¥è¿è¡Œæ—¶ï¼ˆè€Œä¸æ˜¯è¢«å½“ä½œæ¨¡å—å¯¼å…¥æ—¶ï¼‰ï¼Œæ‰æ‰§è¡Œ `main()` å‡½æ•°â€ã€‚

-----

å¸Œæœ›è¿™ä¸ªæå…¶è¯¦ç»†çš„åˆ†è§£èƒ½å¸®åŠ©ä½ ç†è§£è¿™æ®µä»£ç çš„æ¯ä¸€æ­¥ï¼

ä½ æ˜¯å¦æƒ³æ·±å…¥äº†è§£å…¶ä¸­æŸä¸€ä¸ªç‰¹å®šå‡½æ•°ï¼ˆæ¯”å¦‚ `_best_split_feature` é‡Œçš„ `gain` å…¬å¼ï¼‰çš„æ•°å­¦åŸç†ï¼Ÿ